# 大语言模型面试知识点

## 1. LLMs Tokenizer

### 1.1 Byte-Pair Encoding

字节对编码 (Byte-Pair Encoding, BPE) 最初是作为一种压缩文本的算法开发的，后来被 OpenAI 用于预训练 GPT 模型时的分词 (tokenization) 过程。现在，许多 Transformer 模型都在使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa 。

### 1.2 Byte-Level Encoding

### 1.3 WordPiece

WordPiece，从名字好理解，它是一种子词粒度的 tokenize 算法 subword tokenization algorithm，很多著名的 Transformers 模型，比如 BERT / DistilBERT / Electra 都使用了它。

它的原理非常接近 BPE，不同之处在于它做合并时，并不是直接找最高频的组合，而是找能够最大化训练数据似然的 merge 。

### 1.4 Unigram

与 BPE 或者 WordPiece 不同，Unigram 的算法思想是从一个巨大的词汇表出发，再逐渐删除 trim down 其中的词汇，直到 size 满足预定义。

初始的词汇表可以采用所有预分词器分出来的词，再加上所有高频的子串。每次从词汇表中删除词汇的原则是使预定义的损失最小。

### 1.5 SentencePiece

SentencePiece 则是基于 `无监督学习` 的，用的是一种称为 Masked Language Modeling (MLM) 的算法。MLM 的基本思想是：将文本中的部分词语或符号进行遮蔽，然后让模型预测被遮蔽的词语或符号，通过这种方式，模型可以学习文本的语义和结构。

顾名思义，它是把一个句子看作一个整体，再拆成片段，而没有保留天然的词语的概念。一般地，它把空格 space 也当作一种特殊字符来处理，再用 BPE 或者 Unigram 算法来构造词汇表。

## 2. LLMs Word Embeddings

Word embedding 是自然语言处理（NLP）中用于将单词转换为数值向量表示的技术。存在多种不同的word embedding 模型，以下是一些常见的模型：

1. **Word2Vec**：由 Google 开发，包括 CBOW（Continuous Bag of Words）和 Skip-Gram 两种模型。

2. **GloVe (Global Vectors for Word Representation)**：利用全局词-词共现统计数据来学习词向量。

3. **FastText**：由 Facebook 开发，与 Word2Vec 类似，但将单词表示为字符 n-gram 的向量。

4. **BERT (Bidirectional Encoder Representations from Transformers)**：由 Google 开发，使用 Transformer 架构来学习深层的双向表示。

5. **ELMo (Embeddings from Language Models)**：由华盛顿大学开发，使用深度双向语言模型来生成词向量。

6. **ULMFiT (Unsupervised Language Model Pre-training for Text Classification and Representation)**：由 Salesforce 研究团队开发，用于文本分类和表示学习。

7. **Flair**：一个基于 PyTorch 的 NLP 库，它提供了基于字符的词嵌入，并且可以与 Word2Vec、GloVe 等其他嵌入组合。

8. **Sent2Vec**：由 Andrew M. Dai 和 Christopher Manning 提出，用于生成句子和短语的嵌入。

9. **T-SNE (t-distributed Stochastic Neighbor Embedding)**：虽然不是专门的 word embedding 模型，但T-SNE常用于可视化高维数据，包括词向量。

10. **Siamese Networks**：用于学习单词或句子对的相似性，可以用于生成 word embedding 。

11. **Meta-Embeddings**：结合多种不同的 word embedding 技术来提高性能。

12. **Contextual Embeddings**：基于特定上下文生成单词的嵌入，如 BERT、GPT（Generative Pre-trained Transformer）等。

这些模型各有优势和特点，适用于不同的 NLP 任务和场景。随着 NLP 领域的发展，新的 word embedding 模型也在不断地被提出和改进。

## 3. 如何评估一个 word embedding 模型的质量？

评估一个 word embedding 模型的质量通常涉及以下几个方面：

1. **语义相似度（Semantic Similarity）**：

   - **相似度评分**：使用像 WordSim-353 或 MEN 这样的语义相似度数据集，这些数据集提供了人工标注的词对及其相似度评分。模型需要预测这些词对的相似度，然后与人工评分进行比较。
   - **余弦相似度**：计算词向量之间的余弦相似度，并与人工评分进行相关性分析。

2. **类比任务（Analogy Task）**：

   - **词类比**：评估模型是否能够捕捉到类似“国王：王后”这样的词之间的关系。这通常通过解决类比问题（如：man is to woman as king is to ?）来完成。

3. **上下文准确性（Contextual Accuracy）**：

   - **上下文词替换**：在一个句子中替换一个词，并使用模型预测最可能的替换词。这可以通过像 COCOA 这样的数据集来评估。

4. **零样本学习（Zero-Shot Learning）**：

   - **类别识别**：评估模型是否能够识别不同类别的图像，这通常通过检查模型是否能够将单词正确地分类到预定义的类别中来完成。

5. **迁移学习（Transfer Learning）**：

   - **下游任务性能**：在一个或多个NLP任务（如文本分类、情感分析、问答系统等）上使用预训练的词向量，并评估这些任务的性能。如果预训练的词向量能够提高这些任务的性能，那么可以认为词向量的质量较高。

6. **定性分析（Qualitative Analysis）**：

   - **最近邻查询**：检查一个词的最近邻词是否在语义上相关，这可以提供模型性能的直观感受。

7. **定量分析（Quantitative Analysis）**：

   - **标准化测试集**：使用标准化的测试集来评估模型的性能，这些测试集通常包含了多种类型的任务。

8. **计算效率（Computational Efficiency）**：

   - **训练和推理时间**：评估模型训练和推理的时间复杂度，对于大规模应用来说，计算效率是一个重要的考量因素。

9. **可扩展性（Scalability）**：

   - **处理大规模数据集**：评估模型在大规模数据集上的表现，以及是否能够有效地扩展到更大的词汇表和数据集。

10. **鲁棒性（Robustness）**：

    - **对抗性测试**：评估模型对于对抗性样本（如拼写错误、同义词替换等）的鲁棒性。

11. **多语言和跨领域评估（Cross-lingual and Cross-domain Evaluation）**：

    - **跨语言和跨领域性能**：评估模型在不同语言和不同领域数据上的性能。

通过综合这些评估方法，可以全面地了解 word embedding 模型的质量和适用性。通常，没有单一的评估标准能够涵盖所有方面，因此实践中会根据具体应用场景和需求选择合适的评估方法。


### 2.1 Word2Vec

Word2Vec 是 Google 的 Tomas Mikolov 等人于 2013 年提出的一种用于生成词嵌入（word embeddings）的模型。它从大量文本语料中以无监督方式学习语义，能够将文本中的单词转换为词向量，这些向量能够捕捉单词之间的语义关系，被广泛地应用于自然语言处理中。

Word2Vec 模型的核心思想是通过词语的上下文信息来学习词语的向量表示。主要有两种架构：CBOW 模型（Continuous Bag of Words，连续词袋模型）和 Skip-Gram 模型（跳字模型）。

通过训练一个神经网络模型，使得：

1. **CBOW 模型**：给定一个词语的上下文时，能够预测该词语本身。

   - 输入层：将上下文单词通过词嵌入层转换为向量。
   - 隐藏层：将上下文单词向量求和或取平均。
   - 输出层：通过 softmax 层预测目标单词的概率分布。

2. **Skip-gram 模型**：给定一个词语时，能够预测其上下文。

   - 输入层：将目标单词通过词嵌入层转换为向量。
   - 隐藏层：目标单词的向量保持不变。
   - 输出层：通过 softmax 层预测每个上下文单词的概率分布。

Word2Vec 的训练模型本质上是只具有一个隐含层的神经元网络。它的输入是采用 One-Hot 编码的词汇表向量，它的输出也是 One-Hot 编码的词汇表向量。

一般神经网络语言模型在预测的时候，输出的是预测目标词的概率，也就是说我每一次预测都要基于全部的数据集进行计算，这无疑会带来很大的时间开销。不同于其他神经网络，Word2Vec 提出两种加快训练速度的方式，一种是 Hierarchical softmax ，另一种是 Negative Sampling 。

**训练过程**

无论是 CBOW 还是 Skip-Gram 模型，训练过程都涉及到优化一个目标函数，这个函数通常是负采样的 softmax 损失函数。训练的目标是最小化预测误差，即最小化模型输出的概率分布和真实单词出现的概率分布之间的差异。

**词嵌入向量**

训练完成后，每个单词都会被映射到一个高维空间中的向量。这些向量捕捉了单词之间的语义和句法关系，例如 “国王” 和 “王后” 的向量之间的距离会比 “国王” 和 “椅子” 的向量之间的距离更近。

**优势**

- **效率**：Word2Vec 训练速度快，适合大规模数据集。
- **灵活性**：可以用于多种 NLP 任务，如文本分类、情感分析等。
- **泛化能力**：能够捕捉单词之间的复杂关系。

Word2Vec 是自然语言处理领域的一个重要里程碑，为后续的词嵌入技术如 GloVe 和 BERT 等奠定了基础。

## x. 引用

- LLMs Tokenizer

  - [大模型分词：sentencepiece vs tiktoken](https://zhuanlan.zhihu.com/p/691609961)

  - [BPE、WordPiece 和 SentencePiece](https://www.jianshu.com/p/d4de091d1367)

  - [Byte-Pair Encoding 分词算法速读](https://zhuanlan.zhihu.com/p/701869443)

  - [Byte-Pair Encoding(BPE) 分词算法详解](https://zhuanlan.zhihu.com/p/716655053)

  - [Byte-Pair Encoding 算法超详细讲解](https://www.jianshu.com/p/865b741f7b96)

- LLMs Word Embeddings

  - [一文读懂：词向量 Word2Vec](https://zhuanlan.zhihu.com/p/371147732)

- 1111111
